{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b274631b4726409fb21af641b8c1c86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are running the model on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import torch\n",
    "from threading import Thread\n",
    "from transformers import TextIteratorStreamer, MllamaForCausalLM, AutoTokenizer\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## Set up LLM\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "model = MllamaForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "print(f\"You are running the model on: {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_text(file_path):\n",
    "    full_text = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            full_text += page.extract_text() + \"\\n\\n\"\n",
    "    return full_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_stream(prompt, max_new_tokens=1024):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    try:\n",
    "        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "        \n",
    "        generation_kwargs = dict(\n",
    "            inputs,\n",
    "            streamer=streamer,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.7,\n",
    "            temperature=0.2,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        generation_kwargs['eos_token_id'] = tokenizer.encode(\"</explanation>\")[-1]\n",
    "\n",
    "        # Start the generation in a separate thread\n",
    "        thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "        thread.start()\n",
    "\n",
    "        generated_text = \"\"\n",
    "        for new_text in streamer:\n",
    "            generated_text += new_text\n",
    "\n",
    "        thread.join()  # Wait for the generation to finish\n",
    "        \n",
    "        # Clear CUDA cache\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return generated_text\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during text generation: {str(e)}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Ensure we always clear the inputs tensor\n",
    "        del inputs\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, chunk_size=8192, max_chunks=10):\n",
    "    # Tokenize the full text\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False, truncation=False)\n",
    "    \n",
    "    # Process the text in chunks\n",
    "    chunk_summaries = []\n",
    "    for i in range(0, min(len(tokens), chunk_size * max_chunks), chunk_size):\n",
    "        chunk_tokens = tokens[i:i+chunk_size]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        \n",
    "        prompt = f\"Summarize the following excerpt from a scientific paper (part {len(chunk_summaries)+1}):\\n\\n{chunk_text}\\n\\nBrief summary:\"\n",
    "        chunk_summary = generate_text_stream(prompt, max_new_tokens=200)\n",
    "        if chunk_summary:\n",
    "            chunk_summaries.append(chunk_summary)\n",
    "\n",
    "    # Generate final concise summary\n",
    "    final_summary_prompt = \"Based on the following summaries of different parts of a scientific paper, provide a concise summary of the entire paper in no more than 10 sentences. Focus on the main points, methodology, and key findings:\\n\\n\" + \"\\n\\n\".join(chunk_summaries) + \"\\n\\nConcise 10-sentence summary:\"\n",
    "    final_summary = generate_text_stream(final_summary_prompt, max_new_tokens=300)\n",
    "\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66769"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usage\n",
    "pdf_path = '2410.02740v1.pdf'\n",
    "document_text = extract_pdf_text(pdf_path)\n",
    "len(document_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the paper:\n",
      " \n",
      "The paper explores the role of synthetic captions in pre-training multimodal foundation models, including CLIP, multimodal LLMs, and diffusion models. The authors propose a novel captioning pipeline to generate diverse caption formats tailored to different models. A comprehensive study reveals that a hybrid approach combining synthetic captions and original AltText can outperform the use of synthetic captions alone, improving both alignment and performance. Each model demonstrates preferences for particular caption formats, and the optimal captioning techniques vary across models. The authors also investigate the interaction between synthetic captions and original AltText, analyzing whether a hybrid approach can balance the need for diverse data with the benefits of enhanced image-text alignment. The study provides valuable insights into optimizing captioning strategies for pre-training multimodal foundation models. The authors develop a controllable and human-aligned captioning pipeline to generate various types of captions and conduct extensive pre-training experiments to derive key insights. The results show that both AltText and synthetic captions play crucial roles, with AltText contributing to more diverse information and synthetic captions offering improved image-text alignment. The authors propose two new metrics, CHAIR and CapScore, to evaluate the quality of captions generated by a model. Overall, the study aims to improve the quality of captions generated by models and provide a more comprehensive evaluation framework for multimodal tasks. The findings highlight the importance of synthetic captions in multimodal foundation models and suggest future directions for research.  (Note: I have rewritten the summary to make it more concise and clear, while\n"
     ]
    }
   ],
   "source": [
    "summary = summarize_text(document_text)\n",
    "print(\"Summary of the paper:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "def format_summary(summary, width=80):\n",
    "    \"\"\"Format the summary text to fit within a specified width.\"\"\"\n",
    "    return '\\n'.join(textwrap.wrap(summary, width=width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Concise summary of the paper (max 10 sentences):**\n",
       "\n",
       "  The paper explores the role of synthetic captions in pre-training multimodal\n",
       "foundation models, including CLIP, multimodal LLMs, and diffusion models. The\n",
       "authors propose a novel captioning pipeline to generate diverse caption formats\n",
       "tailored to different models. A comprehensive study reveals that a hybrid\n",
       "approach combining synthetic captions and original AltText can outperform the\n",
       "use of synthetic captions alone, improving both alignment and performance. Each\n",
       "model demonstrates preferences for particular caption formats, and the optimal\n",
       "captioning techniques vary across models. The authors also investigate the\n",
       "interaction between synthetic captions and original AltText, analyzing whether a\n",
       "hybrid approach can balance the need for diverse data with the benefits of\n",
       "enhanced image-text alignment. The study provides valuable insights into\n",
       "optimizing captioning strategies for pre-training multimodal foundation models.\n",
       "The authors develop a controllable and human-aligned captioning pipeline to\n",
       "generate various types of captions and conduct extensive pre-training\n",
       "experiments to derive key insights. The results show that both AltText and\n",
       "synthetic captions play crucial roles, with AltText contributing to more diverse\n",
       "information and synthetic captions offering improved image-text alignment. The\n",
       "authors propose two new metrics, CHAIR and CapScore, to evaluate the quality of\n",
       "captions generated by a model. Overall, the study aims to improve the quality of\n",
       "captions generated by models and provide a more comprehensive evaluation\n",
       "framework for multimodal tasks. The findings highlight the importance of\n",
       "synthetic captions in multimodal foundation models and suggest future directions\n",
       "for research.  (Note: I have rewritten the summary to make it more concise and\n",
       "clear, while"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For Jupyter Notebook, you can use display() for richer output\n",
    "from IPython.display import display, Markdown\n",
    "formatted_summary = format_summary(summary)\n",
    "display(Markdown(f\"**Concise summary of the paper (max 10 sentences):**\\n\\n{formatted_summary}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
